import os
import tempfile

import chromadb
import ollama
import streamlit as st
from chromadb.utils.embedding_functions.ollama_embedding_function import (
    OllamaEmbeddingFunction,
)
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import CrossEncoder
from streamlit.runtime.uploaded_file_manager import UploadedFile


import subprocess
import requests
import streamlit as st


import os
os.environ["OLLAMA_GPU_DRIVER"] = "cpu"


def diagnose_ollama():
    """Comprehensive Ollama diagnosis"""
    st.header("ğŸ”§ Ollama Diagnosis")
    
    # Check if Ollama process is running
    try:
        result = subprocess.run(["tasklist", "/fi", "imagename eq ollama.exe"], 
                              capture_output=True, text=True)
        if "ollama.exe" in result.stdout:
            st.success("âœ… Ollama process is running")
        else:
            st.error("âŒ Ollama process NOT found")
            st.info("Start Ollama with: `ollama serve`")
            return False
    except:
        st.warning("âš ï¸ Could not check Ollama process")
    
    # Check API connectivity
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            st.success("âœ… Ollama API is responding")
            models = response.json().get("models", [])
            if models:
                st.write("ğŸ“‹ Loaded models:")
                for model in models:
                    st.write(f"  - {model['name']}")
            else:
                st.warning("âš ï¸ No models loaded")
            return True
        else:
            st.error(f"âŒ Ollama API error: {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        st.error("âŒ Cannot connect to Ollama API")
        st.info("Make sure Ollama is running: `ollama serve`")
        return False
    except Exception as e:
        st.error(f"âŒ Ollama check failed: {str(e)}")
        return False

def restart_ollama():
    """Restart Ollama service"""
    try:
        # Kill existing Ollama processes
        subprocess.run(["taskkill", "/f", "/im", "ollama.exe"], 
                      capture_output=True)
        
        # Start Ollama
        subprocess.Popen(["ollama", "serve"], 
                        stdout=subprocess.DEVNULL, 
                        stderr=subprocess.DEVNULL)
        
        st.success("ğŸ”„ Ollama restarted! Waiting for it to start...")
        time.sleep(5)  # Wait for Ollama to start
        return True
    except Exception as e:
        st.error(f"âŒ Failed to restart Ollama: {str(e)}")
        return False
    

system_prompt = """
# Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…ØªØ®ØµØµ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù‚ÙˆØ§Ù†ÛŒÙ†ØŒ Ù…Ù‚Ø±Ø±Ø§Øª Ùˆ Ø§Ø³Ù†Ø§Ø¯ Ø­Ù‚ÙˆÙ‚ÛŒ Ù‡Ø³ØªÛŒØ¯.
# ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ Ø´Ù…Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§Ø¨ØªØ¯Ø§ Ù¾Ø±Ø³Ø´ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø¯Ø± <<ØµÙˆØ±Øª Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ ÛŒØ§ ØºÛŒØ±Ø±Ø³Ù…ÛŒ Ø¨ÙˆØ¯Ù†>>ØŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±Ø³Ù…ÛŒ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯. 
# Ø³Ù¾Ø³ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Â«Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡Â» Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ØŒ Ù¾Ø§Ø³Ø® Ú©ÙˆØªØ§Ù‡ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø´ÙØ§Ù Ø¨Ø¯Ù‡ÛŒØ¯.

# Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„â€ŒÙ‡Ø§:
# 1. ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.
# 2.Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ Ú©ÙˆØªØ§Ù‡ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø´ÙØ§Ù Ø¨Ø§Ø´Ø¯. Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.
# 3. Ø§Ú¯Ø± Ù¾Ø±Ø³Ø´ Ø´Ø§Ù…Ù„ Ú†Ù†Ø¯ Ø¨Ø®Ø´ Ø§Ø³ØªØŒ Ù‡Ø± Ø¨Ø®Ø´ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ùˆ Ø´ÙØ§Ù Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.
# 4. Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù‡Ø± Ù¾Ø§Ø³Ø®ØŒ Ù…Ø­Ù„ Ø¯Ù‚ÛŒÙ‚ Ø§Ø³ØªÙ†Ø§Ø¯ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯ (Ù…Ø§Ù†Ù†Ø¯: Ù…Ø§Ø¯Ù‡ØŒ Ø¨Ù†Ø¯ØŒ ØªØ¨ØµØ±Ù‡ØŒ ØµÙØ­Ù‡ ÛŒØ§ Ø´Ù…Ø§Ø±Ù‡ Ø¨Ø®Ø´).
# 5. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø¯Ø± Ù…ØªÙ† Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªØŒ ØµØ±ÛŒØ­Ø§Ù‹ Ø¨Ú¯ÙˆÛŒÛŒØ¯ Ú©Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.
# 6. Ø²Ø¨Ø§Ù† Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ Ø³Ø§Ø¯Ù‡ Ùˆ Ø±Ø³Ù…ÛŒ Ø¨Ø§Ø´Ø¯ Ùˆ Ø§Ø² Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ø­Ù‚ÙˆÙ‚ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ Ø¯Ù‚ÛŒÙ‚ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯.
# 7. Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø±Ø§Ø³Øª Ø¨Ù‡ Ú†Ù¾ Ùˆ Ø¨Ø¯ÙˆÙ† ØºÙ„Ø· Ø§Ù…Ù„Ø§ÛŒÛŒ Ø¨Ø§Ø´Ø¯.
# 8. Ù¾Ø§Ø³Ø® Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù‚ÙˆØ§Ù†ÛŒÙ† Ù†Ú¯Ø§Ø±Ø´ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¯Ø±Ø³Øª Ø¨Ø§Ø´Ø¯.
# 9. Ø§Ø² ÙØ±Ø¶ÛŒØ§Øª ÛŒØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯Ø› ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.
# 10. Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ø§Ø¯Ù‡ØŒ Ø¨Ù†Ø¯ØŒ ØªØ¨ØµØ±Ù‡ ÛŒØ§ ØµÙØ­Ù‡ ÙÙ‚Ø· Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø¨Ø®Ø´ Â«Ø§Ø³ØªÙ†Ø§Ø¯Â» Ø°Ú©Ø± Ø´ÙˆØ¯ Ùˆ Ù†Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ù…ØªÙ† Ù¾Ø§Ø³Ø® ØªÚ©Ø±Ø§Ø± Ø´ÙˆØ¯.
# 11. Ù‚Ø¨Ù„ Ø§Ø² Ù†ÙˆØ´ØªÙ† Â«Ø§Ø³ØªÙ†Ø§Ø¯Â»ØŒ Ù…Ø·Ù…Ø¦Ù† Ø´Ùˆ Ú©Ù‡ Ù…Ø­Ù„ Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø¯Ø± Ù…ØªÙ† Ø²Ù…ÛŒÙ†Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø§Ú¯Ø± Ù…Ø·Ù…Ø¦Ù† Ù†ÛŒØ³ØªÛŒØŒ Ø¨Ù†ÙˆÛŒØ³: Â«Ø§Ø³ØªÙ†Ø§Ø¯ Ø¯Ù‚ÛŒÙ‚ Ù…Ø´Ø®Øµ Ù†ÛŒØ³ØªÂ».
# 12. Ø§Ø² Ø¯Ø§Ù†Ø´ Ù‚Ø¨Ù„ÛŒ ÛŒØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒØ¯.
# 13. Ø§Ú¯Ø± Ø¹Ø¯Ø¯ÛŒ Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.

# Ù¾Ø±Ø³Ø´: ØªØ¹Ø±ÛŒÙ Ø¯Ø§Ù†Ø´Ú†Ùˆ Ú†ÛŒØ³ØªØŸ 
# ÙØ±Ø¯ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± ÛŒÚ©ÛŒ Ø§Ø² Ø¯ÙˆØ±Ù‡ Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø§Ø¨Ø± Ø¶ÙˆØ§Ø¨Ø· Ù…Ø¹ÛŒÙ† Ù¾Ø°ÛŒØ±ÙØªÙ‡ Ø´Ø¯Ù‡ØŒ Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ø±Ø¯Ù‡ Ùˆ Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ØªØ­ØµÛŒÙ„ Ø§Ø³Øª.

# Ù‚Ø§Ù„Ø¨ Ù¾Ø§Ø³Ø®:
# -Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø®ØªØµØ± Ùˆ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø´ÙØ§Ù 
# - Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ Â«Ø§Ø³ØªÙ†Ø§Ø¯:Â» Ù…Ø­Ù„ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø¯Ø± Ù…ØªÙ† Ø±Ø§ Ø¨ÛŒØ§ÙˆØ±ÛŒØ¯.

# Ù…Ø«Ø§Ù„ Ù‚Ø§Ù„Ø¨:
# Ù¾Ø§Ø³Ø®: [Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡ Ù¾Ø§Ø³Ø®]
#  ÛŒØ§ Ù…Ø§Ø¯Ù‡ ÛŒØ§ ØªØ¨ØµØ±Ù‡ XØŒ Ø¨Ù†Ø¯ YØŒ ØµÙØ­Ù‡ Z: Ø§Ø³ØªÙ†Ø§Ø¯"""




def process_document(uploaded_file: UploadedFile) -> list[Document]:
    """Processes an uploaded PDF file by converting it to text chunks.

    Takes an uploaded PDF file, saves it temporarily, loads and splits the content
    into text chunks using recursive character splitting.

    Args:
        uploaded_file: A Streamlit UploadedFile object containing the PDF file

    Returns:
        A list of Document objects containing the chunked text from the PDF

    Raises:
        IOError: If there are issues reading/writing the temporary file
    """
    # Store uploaded file as a temp file
    with tempfile.NamedTemporaryFile("wb", suffix=".pdf", delete=False) as temp_file:
        temp_file.write(uploaded_file.read())
        temp_file_path = temp_file.name


    try:     
        loader = PyMuPDFLoader(temp_file_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            separators=["\n\n", "\n", ".", "?", "!", " ", "","ØªØ¨ØµØ±Ù‡","Ù…Ø§Ø¯Ù‡","Ø¨Ù†Ø¯"],
        )
        return text_splitter.split_documents(docs)
    
    finally:
        # This ensures the file gets deleted even if there's an error
        try:
            os.unlink(temp_file_path)
        except PermissionError:
            # If file is still in use, just skip deletion
            pass


import chromadb
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction

def get_ollama_embedding_function():
    """Creates and returns an Ollama embedding function configuration.
    
    Returns:
        OllamaEmbeddingFunction: Configured embedding function for Persian model
    """
    return OllamaEmbeddingFunction(
        url="http://localhost:11434/api/embeddings",
        model_name="persian-embed-hedariAI-q4:latest",
    )

def get_vector_collection() -> chromadb.Collection:
    """Gets or creates a ChromaDB collection for vector storage.

    Creates an Ollama embedding function using the persian-hedariAI-q8:latest model and initializes
    a persistent ChromaDB client. Returns a collection that can be used to store and
    query document embeddings.

    Returns:
        chromadb.Collection: A ChromaDB collection configured with the Ollama embedding
            function and cosine similarity space.
    """
    ollama_ef = get_ollama_embedding_function()
    chroma_client = chromadb.PersistentClient(path="./demo-rag-chroma")
    
    return chroma_client.get_or_create_collection(
        name="rag_app",
        embedding_function=ollama_ef,
        metadata={"hnsw:space": "ip"},
    )



def add_to_vector_collection(documents, collection_name):
    """Add documents to the vector collection in batches.
    
    Args:
        documents: List of documents to add
        collection_name: Name of the collection to add documents to
    """
  
    # Use the existing collection
    collection = get_vector_collection()
    
    # Prepare documents in smaller batches
    batch_size = 10
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        
        # Extract content and metadata
        documents_batch = [doc.page_content for doc in batch_docs]
        metadatas_batch = [doc.metadata for doc in batch_docs]
        ids_batch = [f"{collection_name}_{i + j}" for j in range(len(batch_docs))]
        
        try:
            # Upsert with retry logic
            collection.upsert(
                documents=documents_batch,
                metadatas=metadatas_batch,
                ids=ids_batch
            )
            print(f"Successfully upserted batch {i//batch_size + 1}")
            
        except Exception as e:
            print(f"Error upserting batch {i//batch_size + 1}: {str(e)}")
            # Continue with next batch instead of failing completely
            continue


import time
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(2),  # Only retry twice
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(httpx.ReadTimeout)
)
def query_collection(prompt: str, n_results: int = 2, timeout = 60 ):
    """Queries the vector collection with a given prompt to retrieve relevant documents.

    Args:
        prompt: The search query text to find relevant documents.
        n_results: Maximum number of results to return. Defaults to 10.

    Returns:
        dict: Query results containing documents, distances and metadata from the collection.

    Raises:
        ChromaDBError: If there are issues querying the collection.
    """
    try:
        collection = get_vector_collection()
        results = collection.query(query_texts=[prompt], n_results=n_results)
        return results
    except httpx.ReadTimeout:
        st.error("â° Ollama embedding generation timed out. Server might be busy.")
        return None
    except Exception as e:
        st.error(f"âŒ Query error: {str(e)}")
        return None

def call_llm(context: str, prompt: str):
    """Calls the language model with context and prompt to generate a response.

    Uses Ollama to stream responses from a language model by providing context and a
    question prompt. The model uses a system prompt to format and ground its responses appropriately.

    Args:
        context: String containing the relevant context for answering the question
        prompt: String containing the user's question

    Yields:
        String chunks of the generated response as they become available from the model

    Raises:
        OllamaError: If there are issues communicating with the Ollama API
    """
    response = ollama.chat(
        model="gemma3:1b",
        stream=True,
        messages=[
            {
                "role": "system",
                "content": system_prompt,
            },
            {
                "role": "user",
                "content": f"Context: {context}, Question: {prompt}",
            },
        ],
    )
    for chunk in response:
        if chunk["done"] is False:
            yield chunk["message"]["content"]
        else:
            break


# def re_rank_cross_encoders(documents: list[str]) -> tuple[str, list[int]]:
#     """Re-ranks documents using a cross-encoder model for more accurate relevance scoring.

#     Uses the MS MARCO MiniLM cross-encoder model to re-rank the input documents based on
#     their relevance to the query prompt. Returns the concatenated text of the top 3 most
#     relevant documents along with their indices.

#     Args:
#         documents: List of document strings to be re-ranked.

#     Returns:
#         tuple: A tuple containing:
#             - relevant_text (str): Concatenated text from the top 3 ranked documents
#             - relevant_text_ids (list[int]): List of indices for the top ranked documents

#     Raises:
#         ValueError: If documents list is empty
#         RuntimeError: If cross-encoder model fails to load or rank documents
#     """
#     relevant_text = ""
#     relevant_text_ids = []

#     encoder_model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
#     ranks = encoder_model.rank(prompt, documents, top_k=3)
#     for rank in ranks:
#         relevant_text += documents[rank["corpus_id"]]
#         relevant_text_ids.append(rank["corpus_id"])

#     return relevant_text, relevant_text_ids


if __name__ == "__main__":
    # Document Upload Area
    with st.sidebar:
        st.set_page_config(page_title="RAG Question Answer")

        if st.button("Diagnose Ollama"):
            diagnose_ollama()
            
        if st.button("Restart Ollama"):
            restart_ollama()

        uploaded_file = st.file_uploader(
            "**ğŸ“‘ Upload PDF files for QnA**", type=["pdf"], accept_multiple_files=False
        )

        process = st.button(
            "âš¡ï¸ Process",
        )
        if uploaded_file and process:
            normalize_uploaded_file_name = uploaded_file.name.translate(
                str.maketrans({"-": "_", ".": "_", " ": "_"})
            )
            all_splits = process_document(uploaded_file)
            add_to_vector_collection(all_splits, normalize_uploaded_file_name)

    # Question and Answer Area
    st.header("ğŸ—£ï¸ RAG Question Answer")
    prompt = st.text_area("**Ask a question related to your document:**")
    ask = st.button(
        "ğŸ”¥ Ask",
    )

    if ask and prompt:
        results = query_collection(prompt)
        context = results.get("documents")[0]
        # relevant_text, relevant_text_ids = re_rank_cross_encoders(context)
        response = call_llm(context=context, prompt=prompt)
        st.write_stream(response)

        with st.expander("See retrieved documents"):
            st.write(results)

        # with st.expander("See most relevant document ids"):
        #     st.write(relevant_text_ids)
        #     st.write(relevant_text)